{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7cb9e5ae",
      "metadata": {
        "id": "7cb9e5ae"
      },
      "source": [
        "###  Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Hgxh6mNIfATI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgxh6mNIfATI",
        "outputId": "215fdd77-03b3-4964-8f96-c42147020b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pyspellchecker\n",
            "Successfully installed Pyspellchecker-0.8.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: kaggle in /opt/anaconda3/lib/python3.9/site-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /opt/anaconda3/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /opt/anaconda3/lib/python3.9/site-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /opt/anaconda3/lib/python3.9/site-packages (from kaggle) (1.26.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /opt/anaconda3/lib/python3.9/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests->kaggle) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->kaggle) (3.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install Pyspellchecker\n",
        "!pip3 install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c5ab5a19",
      "metadata": {
        "id": "c5ab5a19"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from spellchecker import SpellChecker\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b2f282",
      "metadata": {
        "id": "b8b2f282"
      },
      "source": [
        "### Initializing NLTK Resources and Preprocessing Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7e17ec35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e17ec35",
        "outputId": "c96f1183-e9eb-4243-f8db-3fc70bd81adb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/siva/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/siva/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/siva/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/siva/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "#download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#initialize stemmer, lemmatizer, and spell checker\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "spell = SpellChecker()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cfbb500",
      "metadata": {
        "id": "4cfbb500"
      },
      "source": [
        "###  Loading the Dataset From Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db10d328",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "db10d328",
        "outputId": "cf7eee97-1dbe-46c9-9984-e6aed1ea3015"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "#download the dataset using Kaggle API\n",
        "os.system(\"kaggle datasets download -d sivameenakshi/Online Football Comments\")\n",
        "\n",
        "#define the dataset name\n",
        "dataset_name = \"Online Football Comments\"\n",
        "\n",
        "#unzip the downloaded file\n",
        "zip_path = f\"{dataset_name}.zip\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/Users/siva/Downloads/\")  #extract to 'dataset' folder\n",
        "    print(\"Dataset unzipped successfully!\")\n",
        "else:\n",
        "    print(\"File not found. Please check if the dataset was downloaded correctly.\")\n",
        "\n",
        "df = pd.read_csv('/Users/siva/Downloads/all_comments.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce0eeb9b",
      "metadata": {
        "id": "ce0eeb9b"
      },
      "source": [
        "### Identifying and Correcting Misspelled Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf4a161",
      "metadata": {
        "id": "faf4a161"
      },
      "outputs": [],
      "source": [
        "#load the dataset\n",
        "#df = pd.read_csv('/Users/siva/Downloads/all_comments.csv')\n",
        "\n",
        "#create a list of all words in the dataset\n",
        "all_words = []\n",
        "df['Comment'].dropna().apply(lambda x: all_words.extend(word_tokenize(x.lower())))\n",
        "all_words = [word for word in all_words if word.isalpha()]  # Filter out non-alphabetic\n",
        "\n",
        "#identify the unique words and their frequencies\n",
        "word_freq = Counter(all_words)\n",
        "\n",
        "#identify misspelled words\n",
        "misspelled_words = spell.unknown(word_freq.keys())\n",
        "\n",
        "#create a dictionary of corrected spellings\n",
        "corrected_spellings = {word: spell.correction(word) for word in misspelled_words}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0ebb79",
      "metadata": {
        "id": "db0ebb79"
      },
      "source": [
        "### Replacing Misspelled Words in Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87434d98",
      "metadata": {
        "id": "87434d98"
      },
      "outputs": [],
      "source": [
        "def replace_misspelled(text, corrections):\n",
        "    if text is None:\n",
        "        return text  #return None if input is None\n",
        "    words = word_tokenize(text.lower())\n",
        "    corrected_text = []\n",
        "    for word in words:\n",
        "        #check if the word is in corrections, and it's not None\n",
        "        corrected_word = corrections.get(word)\n",
        "        if corrected_word is None:\n",
        "            corrected_word = word\n",
        "        corrected_text.append(corrected_word)\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "#apply the function to the DataFrame\n",
        "df['Corrected_Comment'] = df['Comment'].apply(lambda x: replace_misspelled(x, corrected_spellings) if pd.notnull(x) else x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70707232",
      "metadata": {
        "id": "70707232"
      },
      "source": [
        "###  Preprocessing Text Data (Tokenization, Stemming, Lemmatization, Removing Stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7de9e7b",
      "metadata": {
        "id": "a7de9e7b"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    #check if the text is None\n",
        "    if text is None:\n",
        "        return ''\n",
        "\n",
        "    #tokenization and lowercasing\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    #remove punctuation and non-alphabetic characters\n",
        "    tokens = [t for t in tokens if t.isalpha()]\n",
        "\n",
        "    #stemming and Lemmatization\n",
        "    stemmed = [stemmer.stem(t) for t in tokens if t is not None]\n",
        "    lemmatized = [lemmatizer.lemmatize(t, pos='v') for t in stemmed if t is not None]\n",
        "\n",
        "    #remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [w for w in lemmatized if not w in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "#apply the preprocessing function to the DataFrame\n",
        "df['Processed_Comment'] = df['Corrected_Comment'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48510e59",
      "metadata": {
        "id": "48510e59"
      },
      "source": [
        "### Saving the Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c1c30a",
      "metadata": {
        "id": "83c1c30a",
        "outputId": "30a36117-ccb8-44b6-bd02-d7dff6fe42a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing complete. The processed comments are saved at: /Users/snehamondal/Desktop/Data/processed_comments.csv\n"
          ]
        }
      ],
      "source": [
        "#save the processed dataframe to a new CSV file\n",
        "processed_csv_path = '/Users/siva/Downloads/processed_comments.csv'\n",
        "df.to_csv(processed_csv_path, index=False)\n",
        "\n",
        "print(\"Processing complete. The processed comments are saved at:\", processed_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93435745",
      "metadata": {
        "id": "93435745",
        "outputId": "80bd6248-fa5a-49a1-a54f-684c5e8b9b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       VideoID                                            Comment  \\\n",
            "0  bmR98lYurtc  Forest don’t play premier league football, the...   \n",
            "1  bmR98lYurtc                              Hope bren&#39;s okay.   \n",
            "2  bmR98lYurtc  The referee is such a weird guy that he needs ...   \n",
            "3  bmR98lYurtc  Gotta be wary of offences &amp; çards............   \n",
            "4  bmR98lYurtc                               Semangat untuk juara   \n",
            "\n",
            "               Author             Timestamp  \\\n",
            "0    @JackSouth-gm6qr  2023-12-16T21:04:02Z   \n",
            "1            @alae709  2023-12-16T21:03:18Z   \n",
            "2        @junodoh6358  2023-12-16T20:54:08Z   \n",
            "3  @williamwilkes9873  2023-12-16T20:25:49Z   \n",
            "4     @joypradana9237  2023-12-16T19:38:27Z   \n",
            "\n",
            "                                   Corrected_Comment  \\\n",
            "0  forest don ’ i play premier league football , ...   \n",
            "1                        hope been & # 39 ; i okay .   \n",
            "2  the referee is such a weird guy that he needs ...   \n",
            "3  got ta be wary of offences & amp ; cards ........   \n",
            "4                               semangat untuk guard   \n",
            "\n",
            "                                   Processed_Comment  \n",
            "0  forest play premier leagu footbal go get rid c...  \n",
            "1                                          hope okay  \n",
            "2                    refere weird guy need disciplin  \n",
            "3                        get ta wari offenc amp card  \n",
            "4                               semangat untuk guard  \n"
          ]
        }
      ],
      "source": [
        "#load the processed comments from the CSV file\n",
        "processed_comments_path = '/Users/siva/Downloads//processed_comments.csv'\n",
        "df_processed = pd.read_csv(processed_comments_path)\n",
        "\n",
        "#display the first few rows of the dataframe\n",
        "print(df_processed.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ffa0d2",
      "metadata": {
        "id": "27ffa0d2"
      },
      "source": [
        "### Preparing Data for TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f131c1a",
      "metadata": {
        "id": "1f131c1a"
      },
      "outputs": [],
      "source": [
        "#load the preprocessed comments and handle any missing values\n",
        "comments = df['Processed_Comment'].fillna('')  # Replace NaNs with empty strings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fb75fd",
      "metadata": {
        "id": "90fb75fd"
      },
      "source": [
        "### Initializing and Applying TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd40cec",
      "metadata": {
        "id": "0bd40cec"
      },
      "outputs": [],
      "source": [
        "#initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(comments)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "76b3fd89",
      "metadata": {
        "id": "76b3fd89"
      },
      "source": [
        "#### Analyzing TF-IDF Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b85df0",
      "metadata": {
        "id": "08b85df0",
        "outputId": "549a418c-ec7c-4894-c93f-ee65392d80e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            tfidf\n",
            "rid      0.450850\n",
            "cooper   0.428308\n",
            "forest   0.409370\n",
            "premier  0.319684\n",
            "footbal  0.299802\n",
            "leagu    0.281380\n",
            "go       0.255152\n",
            "get      0.235711\n",
            "play     0.231919\n",
            "ростов   0.000000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/snehamondal/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "#retrieve feature names (words) from the vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "#extract and analyze TF-IDF scores for the first document (comment)\n",
        "first_document_vector = tfidf_matrix[0]\n",
        "\n",
        "#convert the TF-IDF results for the first document into a readable DataFrame\n",
        "df_tfidf = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
        "\n",
        "df_tfidf = df_tfidf.sort_values(by=[\"tfidf\"], ascending=False)\n",
        "\n",
        "print(df_tfidf.head(10))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "QrylLc2Xmppd",
      "metadata": {
        "id": "QrylLc2Xmppd"
      },
      "source": [
        "### Bag of Words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62005af1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "62005af1",
        "outputId": "97887e65-5b0d-4cf6-8e23-663abfe08b3a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#initialize CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "#fit and transform the comments\n",
        "bow_matrix = count_vectorizer.fit_transform(comments)\n",
        "\n",
        "#get feature names (words)\n",
        "bow_feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "print('Bag of Words matrix shape:', bow_matrix.shape)\n",
        "print('Sample BoW features:', bow_feature_names[:10])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0nWwSb0FmybP",
      "metadata": {
        "id": "0nWwSb0FmybP"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e297db3f",
      "metadata": {
        "id": "e297db3f"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "#tokenize the preprocessed comments\n",
        "tokenized_comments = [comment.split() for comment in comments]\n",
        "\n",
        "#train a Word2Vec model\n",
        "word2vec_model = Word2Vec(tokenized_comments, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "#view Word2Vec results\n",
        "print('Vocabulary size:', len(word2vec_model.wv.key_to_index))\n",
        "print('Vector for a sample word:', word2vec_model.wv['premier'])  #sample word from the data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
