{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb9e5ae",
   "metadata": {},
   "source": [
    "###  Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ab5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2f282",
   "metadata": {},
   "source": [
    "### Initializing NLTK Resources and Preprocessing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e17ec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/snehamondal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/snehamondal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/snehamondal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/snehamondal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#initialize stemmer, lemmatizer, and spell checker\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbb500",
   "metadata": {},
   "source": [
    "###  Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db10d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "df = pd.read_csv('/Users/siva/Downloads/all_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0eeb9b",
   "metadata": {},
   "source": [
    "### Identifying and Correcting Misspelled Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf4a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all words in the dataset\n",
    "all_words = []\n",
    "df['Comment'].dropna().apply(lambda x: all_words.extend(word_tokenize(x.lower())))\n",
    "all_words = [word for word in all_words if word.isalpha()]  # Filter out non-alphabetic\n",
    "\n",
    "#identify the unique words and their frequencies\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "#identify misspelled words\n",
    "misspelled_words = spell.unknown(word_freq.keys())\n",
    "\n",
    "#create a dictionary of corrected spellings\n",
    "corrected_spellings = {word: spell.correction(word) for word in misspelled_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ebb79",
   "metadata": {},
   "source": [
    "### Replacing Misspelled Words in Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87434d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_misspelled(text, corrections):\n",
    "    if text is None:\n",
    "        return text  #return None if input is None\n",
    "    words = word_tokenize(text.lower())\n",
    "    corrected_text = []\n",
    "    for word in words:\n",
    "        #check if the word is in corrections, and it's not None\n",
    "        corrected_word = corrections.get(word)\n",
    "        if corrected_word is None:\n",
    "            corrected_word = word\n",
    "        corrected_text.append(corrected_word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "#apply the function to the DataFrame\n",
    "df['Corrected_Comment'] = df['Comment'].apply(lambda x: replace_misspelled(x, corrected_spellings) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70707232",
   "metadata": {},
   "source": [
    "###  Preprocessing Text Data (Tokenization, Stemming, Lemmatization, Removing Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7de9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #check if the text is None\n",
    "    if text is None:\n",
    "        return ''\n",
    "\n",
    "    #tokenization and lowercasing\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    #remove punctuation and non-alphabetic characters\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "    #stemming and lemmatization\n",
    "    stemmed = [stemmer.stem(t) for t in tokens if t is not None]\n",
    "    lemmatized = [lemmatizer.lemmatize(t, pos='v') for t in stemmed if t is not None]\n",
    "\n",
    "    #remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [w for w in lemmatized if not w in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#apply the preprocessing function to the DataFrame\n",
    "df['Processed_Comment'] = df['Corrected_Comment'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48510e59",
   "metadata": {},
   "source": [
    "### Saving the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c1c30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. The processed comments are saved at: /Users/snehamondal/Desktop/Data/processed_comments.csv\n"
     ]
    }
   ],
   "source": [
    "#save the processed dataframe to a new CSV file\n",
    "processed_csv_path = '/Users/siva/Downloads/processed_comments.csv'\n",
    "df.to_csv(processed_csv_path, index=False)\n",
    "\n",
    "print(\"Processing complete. The processed comments are saved at:\", processed_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93435745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       VideoID                                            Comment  \\\n",
      "0  bmR98lYurtc  Forest don’t play premier league football, the...   \n",
      "1  bmR98lYurtc                              Hope bren&#39;s okay.   \n",
      "2  bmR98lYurtc  The referee is such a weird guy that he needs ...   \n",
      "3  bmR98lYurtc  Gotta be wary of offences &amp; çards............   \n",
      "4  bmR98lYurtc                               Semangat untuk juara   \n",
      "\n",
      "               Author             Timestamp  \\\n",
      "0    @JackSouth-gm6qr  2023-12-16T21:04:02Z   \n",
      "1            @alae709  2023-12-16T21:03:18Z   \n",
      "2        @junodoh6358  2023-12-16T20:54:08Z   \n",
      "3  @williamwilkes9873  2023-12-16T20:25:49Z   \n",
      "4     @joypradana9237  2023-12-16T19:38:27Z   \n",
      "\n",
      "                                   Corrected_Comment  \\\n",
      "0  forest don ’ i play premier league football , ...   \n",
      "1                        hope been & # 39 ; i okay .   \n",
      "2  the referee is such a weird guy that he needs ...   \n",
      "3  got ta be wary of offences & amp ; cards ........   \n",
      "4                               semangat untuk guard   \n",
      "\n",
      "                                   Processed_Comment  \n",
      "0  forest play premier leagu footbal go get rid c...  \n",
      "1                                          hope okay  \n",
      "2                    refere weird guy need disciplin  \n",
      "3                        get ta wari offenc amp card  \n",
      "4                               semangat untuk guard  \n"
     ]
    }
   ],
   "source": [
    "#load the processed comments from the CSV file\n",
    "processed_comments_path = '/Users/siva/Downloads//processed_comments.csv'\n",
    "df_processed = pd.read_csv(processed_comments_path)\n",
    "\n",
    "#display the first few rows of the dataframe\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ffa0d2",
   "metadata": {},
   "source": [
    "### Preparing Data for TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f131c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the preprocessed comments and handle any missing values\n",
    "comments = df['Processed_Comment'].fillna('')  # Replace NaNs with empty strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb75fd",
   "metadata": {},
   "source": [
    "### Initializing and Applying TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd40cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3fd89",
   "metadata": {},
   "source": [
    "### Analyzing TF-IDF Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b85df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tfidf\n",
      "rid      0.450850\n",
      "cooper   0.428308\n",
      "forest   0.409370\n",
      "premier  0.319684\n",
      "footbal  0.299802\n",
      "leagu    0.281380\n",
      "go       0.255152\n",
      "get      0.235711\n",
      "play     0.231919\n",
      "ростов   0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snehamondal/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#retrieve feature names (words) from the vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "#extract and analyze TF-IDF scores for the first document (comment)\n",
    "first_document_vector = tfidf_matrix[0]\n",
    "\n",
    "#convert the TF-IDF results for the first document into a readable DataFrame\n",
    "df_tfidf = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "\n",
    "df_tfidf = df_tfidf.sort_values(by=[\"tfidf\"], ascending=False)\n",
    "\n",
    "print(df_tfidf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62005af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297db3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
